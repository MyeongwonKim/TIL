### Introduction to PySpark

- Spark: A cluster computing framework for data analytics that can handle Big Data, almost all sorts of queries of all sorts of data types in a lightning fast speed.
- PySpark: A Python API for Spark that lets you harness the simplicity of Python and the power of Apache Spark for Big Data analytics.
- Cluster Computing
  - Benefits: Faster processing, Larger storage, Better data integrity
- Lazy Computation of Spark: Spark does not execute the transformations until some action is encountered or cache is encountered.

#### Spark MLlib
- Motivation: make machine learning scalable and easy

#### Comparison with other skills
- Hadoop MapReduce - only batch processing
- Apache Storm / S4 - only stream processing
- Apache Impala / Tez - only interactive processing
- Neo4j / Giraph - only graph processing
- Spark - Can perform all of the above

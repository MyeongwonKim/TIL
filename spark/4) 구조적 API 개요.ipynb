{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame과 Dataset\n",
    "- 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야하는지 정의하는 지연 연산의 **실행 계획**\n",
    "- 불변성을 가짐\n",
    "- 액션을 호출하면 스파크는 실제로 transformation을 실행하고 결과를 반환함\n",
    "- 스키마: DataFrame의 컬럼명과 데이터 타입을 정의함. 스파크는 자체 데이터 타입을 갖고 있음\n",
    "\n",
    "- Dataset은 scala와 java에서만 지원되며, Dataset의 경우 컴파일 타임에 스키마에 명시된 데이터 타입의 일치여부를 검증함\n",
    "- DataFrame은 런타임이 되어서야 데이터 타입의 일치 여부를 검증함\n",
    "- DataFrame은 Row 타입으로 구성된 Dataset임. 이 형식은 \"연산에 최적화된 인메모리 포맷\"의 내부적 표현 방식임\n",
    "\n",
    "### 컬럼과 로우\n",
    "- 컬럼: 테이블의 컬럼과 동일. 단순 데이터 타입, 배열과 같은 복합 데이터 타입, null을 표현함\n",
    "- 로우: 데이터 레코드. 데이터소스에서 얻거나 직접 생성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [number: bigint]\r\n",
       "res8: Array[org.apache.spark.sql.Row] = Array([10], [11], [12], [13], [14], [15], [16], [17], [18], [19])\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 스파크의 덧셈 연산\n",
    "val df = spark.range(500).toDF(\"number\") //0~499까지의 값이 할당된 500 row의 df 생성\n",
    "df.select(df.col(\"number\")+10).take(10)  //각 로우에 10을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "b: org.apache.spark.sql.types.ByteType.type = ByteType\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// scala에서 스파크 데이터 타입 사용\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val b = ByteType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구조적 API의 실행 과정\n",
    "- DF/DS/SQL을 이용해 코드 작성 &rarr; 논리적 실행 계획 생성 &rarr; 최적화 및 물리적 실행 계획으로 변환 &rarr; 연산 실행\n",
    "- 논리적 실행 계획: catalyst optimizer가 실행 계획을 최적화\n",
    "- 물리적 실행 계획: 논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의. 최종적으로 RDD transformation으로 변환되어 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구조적 API의 기본 기능\n",
    "- 스키마: DataFrame의 컬럼명과 데이터 타입을 정의. 스키마를 데이터에서 읽어오거나 스스로 만들 수도 있음\n",
    "- 사용자는 표현식을 사용하여 컬럼의 선택, 조작, 제거를 수행함. 컬럼은 사실상 표현식으로 레코드의 값을 단순하게 나타내는 논리적 구조임\n",
    "- 표현식: DataFrame 레코드의 여러 값에 대한 transformation 집합\n",
    "- row: 스파크는 레코드를 Row 객체로 표현함. Row 객체는 스키마 정보를 갖고 있지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame 생성\n",
    "val df = spark.read.format(\"json\")\n",
    "  .load(\"c:/SparkDG/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//스키마 출력\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, column, expr}\r\n",
       "res14: org.apache.spark.sql.Column = someColumnName2\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//column 생성: col 함수 사용\n",
    "import org.apache.spark.sql.functions.{col, column, expr}\n",
    "col(\"someColumnName\")\n",
    "expr(\"someColumnName2\")  // expr 함수에 인수로 컬럼 표현식을 넣으면 col 함수와 동일하게 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "myRow: org.apache.spark.sql.Row = [Hello,null,1,false]\r\n",
       "res21: Int = 1\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-08 18:03:57 WARN  Executor:87 - Issue communicating with driver in heartbeater\r\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\r\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\r\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\r\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\r\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\r\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)\r\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:785)\r\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:814)\r\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)\r\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)\r\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\r\n",
      "\tat org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:814)\r\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\r\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\r\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\r\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\r\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\r\n",
      "\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "//row 생성 및 데이터 접근. row 생성시 df와 같은 순서로 값을 명시해야 함\n",
    "import org.apache.spark.sql.Row\n",
    "val myRow = Row(\"Hello\", null, 1, false)\n",
    "\n",
    "myRow.getString(0)\n",
    "myRow.getInt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select 메서드\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스파크의 기본 아키텍쳐\n",
    "- 스파크는 사용 가능한 자원을 파악하기 위해 스파크 standalone, 하둡 YARN, Mesos 같은 클러스터 매니저를 사용함\n",
    "- 연산 처리 과정: 사용자가 클러스터 매니저에 스파크 애플리케이션을 submit함 -> 클러스터 매니저는 필요한 자원을 할당하여 작업을 처리함\n",
    "- 스파크 애플리케이션: 드라이버 프로세스와 익스큐터 프로세스로 구성됨\n",
    "- 드라이버 프로세스: 사용자의 입력에 대한 응답, 전반적인 작업 분석, 배포 및 스케쥴링 담당\n",
    "- 익스큐터 프로세스: 드라이버 프로세스가 할당한 작업을 수행하고 다시 드라이버에 보고함\n",
    "\n",
    "### SparkSession\n",
    "- 스파크 애플리케이션은 SparkSession이라고 불리는 드라이버 프로세스로 제어함\n",
    "- SparkSession 객체는 사용자의 처리 명령을 클러스터에서 실행함\n",
    "- 하나의 SparkSession이 하나의 스파크 애플리케이션에 대응됨\n",
    "\n",
    "### DataFrame\n",
    "- 가장 대표적인 구조적 API\n",
    "- 테이블의 데이터를 row, column으로 단순하게 표현함. 데이터는 클러스터에 분산 저장됨\n",
    "- 파티션: 데이터 병렬 처리를 위해 청크 단위로 나누어진 데이터의 집합 (클러스터의 각 노드에 존재하는 row의 집합)\n",
    "\n",
    "### Transformation\n",
    "- DataFrame을 변경하는 것\n",
    "- 좁은 Transformation: 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침. where 구문 등\n",
    "- 넓은 Transformation: 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침. shuffle시 일어남 (sort() 등)\n",
    "- 지연 실행: 스파크는 연산 명령이 내려지기 전에는 데이터를 수정하지 않고 transformation 실행 계획만 생성함 -> 연산 명령이 떨어진 시점에서 실행 계획을 최적화한 상태로 연산 수행\n",
    "- 실행 계획: transformation의 지향성 비순환 그래프 (directed acyclic graph)이며 action 호출시 결과를 만들어냄. 각 단계마다 불변성을 지닌 새로운 DataFrame이 생성됨\n",
    "- 스파크 UI: 스파크 job의 상태, 환경 설정, 클러스터 상태 등 모니터링 가능. 드라이버 노드의 4040 port로 접속\n",
    "- 사용자는 물리적 데이터를 직접 다루지 않고, 속성값을 변경하여 데이터 처리의 실행 특성을 제어함\n",
    "\n",
    "### Action\n",
    "- 일련의 transformation으로부터 결과를 계산하도록 지시하는 연산 명령\n",
    "- ex) 콘솔에서 데이터를 봄, 각 언어의 native 객체에 데이터를 모음, 출력 데이터 소스에 저장 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] 미국 교통통계국의 항공운항 데이터 분석\n",
    "- semi-structured csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 객체 생성\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VMF5KGF:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29119277fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "- 스키마 정보를 자동으로 알아내는 schema inference 기능 사용 (시간이 걸리기 때문에 실제 운영 환경에서는 직접 스키마를 지정하여 로딩함)\n",
    "- 첫 row를 헤더로 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"c:/SparkDG/data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort (count column 기준)\n",
    "- .sort(): 단지 transformation임으로 실제 데이터에는 아무런 변화는 일어나지 않고 실행 계획만 생성됨\n",
    "- .explain(): 스파크의 쿼리 실행 계획을 보여주는 method\n",
    "- 실행 계획은 역순으로 출력됨. Sort / Exchange / FileScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/c:/SparkDG/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle의 출력 partition 수를 5로 줄이고 sort action을 수행\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame과 SQL\n",
    "- DataFrame을 view로 등록한 후 SQL 쿼리 수행\n",
    "- SQL쿼리는 DataFrame 코드와 같은 실행 계획으로 컴파일되므로 둘 사이의 성능 차이는 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest_country_name='Moldova', count(1)=1),\n",
       " Row(dest_country_name='Bolivia', count(1)=1),\n",
       " Row(dest_country_name='Algeria', count(1)=1),\n",
       " Row(dest_country_name='Turks and Caicos Islands', count(1)=1),\n",
       " Row(dest_country_name='Pakistan', count(1)=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "select dest_country_name, count(1)\n",
    "from flight_data_2015\n",
    "group by dest_country_name\n",
    "\"\"\")\n",
    "\n",
    "sqlWay.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest_country_name='United States', cnt=125),\n",
       " Row(dest_country_name='Taiwan', cnt=1),\n",
       " Row(dest_country_name='Latvia', cnt=1),\n",
       " Row(dest_country_name='Ecuador', cnt=1),\n",
       " Row(dest_country_name='Venezuela', cnt=1),\n",
       " Row(dest_country_name='Hungary', cnt=1),\n",
       " Row(dest_country_name='Martinique', cnt=1),\n",
       " Row(dest_country_name='Croatia', cnt=1),\n",
       " Row(dest_country_name='Cyprus', cnt=1),\n",
       " Row(dest_country_name='Qatar', cnt=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql2 = spark.sql(\"\"\"\n",
    "select dest_country_name, count(*) as cnt\n",
    "from flight_data_2015\n",
    "group by dest_country_name\n",
    "order by cnt desc\n",
    "\"\"\")\n",
    "\n",
    "sql2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대값 구하기\n",
    "spark.sql(\"select max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|dest_country_name|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 목적지 별 count 합계 구하기 (SQL 구문)\n",
    "maxSql = spark.sql(\"\"\"\n",
    "select dest_country_name, sum(count) as destination_total\n",
    "from flight_data_2015\n",
    "group by dest_country_name\n",
    "order by sum(count) desc\n",
    "limit 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 목적지 별 count 합계 구하기 (DataFrame 구문)\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(desc(\"destination_total\"))\\\n",
    "  .limit(5)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame의 변환 흐름\n",
    "- read -> groupBy -> sum -> column rename -> sort -> limit -> collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#59L DESC NULLS LAST], output=[dest_country_name#17,sum(count)#58L])\n",
      "+- *(2) HashAggregate(keys=[dest_country_name#17], functions=[sum(cast(count#19 as bigint))])\n",
      "   +- Exchange hashpartitioning(dest_country_name#17, 5)\n",
      "      +- *(1) HashAggregate(keys=[dest_country_name#17], functions=[partial_sum(cast(count#19 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#17,count#19] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/c:/SparkDG/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [스파크 완벽 가이드] - chapter 2 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
